# NPChat - Near Partner RAG Chatbot

A production-ready RAG (Retrieval-Augmented Generation) chatbot built with Python, Ollama, ChromaDB, and Streamlit. Designed to answer any question about Near Partner â€” its services, values, culture, and team â€” using content scraped from nearpartner.com.

## Features

### Core RAG Pipeline
- **Web Scraper**: Incremental blog post and company page scraping from nearpartner.com
- **Document Ingestion**: Chunking (~1200 chars), embedding, and storage in ChromaDB
- **Semantic Search**: Vector similarity search using Ollama embeddings
- **LLM Generation**: Response generation using local Ollama models (no API keys)

### Advanced ML Features
- **Hybrid Search**: Combines semantic (embedding) search with BM25 keyword search using Reciprocal Rank Fusion (RRF)
- **Query Expansion**: Automatically expands queries with related terms for broader retrieval
- **HyDE**: Hypothetical Document Embedding for improved retrieval on abstract questions
- **Response Caching**: SQLite-based cache to avoid redundant LLM calls
- **Multi-turn Conversation**: Maintains conversation history for follow-up questions
- **Auto-Quality Evaluation**: LLM self-evaluates response confidence (0.0â€“1.0); warns on low-confidence answers

### Automatic Feedback Learning
- **Cache Invalidation**: Removes cached responses on negative feedback
- **Chunk Boosting/Penalizing**: Adjusts retrieval scores based on user feedback
- **Auto-Flagging**: Flags queries with repeated negative feedback for review
- **Query Mapping**: Learns successful query-chunk mappings from positive feedback

### Analytics & Monitoring
- **Query Logging**: Tracks all queries with retrieval scores and response times
- **Feedback Analytics**: Monitors positive/negative feedback trends
- **Knowledge Gap Detection**: Identifies low-score queries indicating missing content
- **Learning Statistics**: Shows chunk adjustments and flagged queries
- **Weekly Reports**: Auto-generated JSON reports saved to `data/reports/`

### Operational Features
- **Rate Limiting**: 30 requests/minute per IP on chat endpoints
- **Prompt Injection Protection**: Input sanitization and system prompt hardening
- **Automatic Scheduling**: Weekly scrape+ingest, daily cache cleanup (APScheduler)
- **Data Export**: Full export of knowledge base, logs, feedback, and cache
- **Security**: Localhost-only API binding, parameterized SQL, no secrets stored

## Tech Stack

| Component | Technology |
|-----------|-----------|
| LLM | Ollama (gemma2:2b, mistral:7b, â€¦) |
| Embeddings | nomic-embed-text via Ollama |
| Vector Store | ChromaDB |
| Keyword Search | BM25 (rank-bm25) |
| Backend API | FastAPI + Uvicorn |
| Frontend | Streamlit |
| Databases | SQLite (analytics, cache, feedback, learning) |
| Scheduler | APScheduler |
| Tests | pytest |

## Installation

### Prerequisites
- Python 3.12+
- [Ollama](https://ollama.ai) installed and running

### Setup

1. **Clone the repository**
```bash
git clone https://github.com/nelsonandreproton/NPChat.git
cd NPChat
```

2. **Create virtual environment**
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Pull Ollama models**
```bash
ollama pull nomic-embed-text
ollama pull gemma2:2b  # or mistral:7b for better quality
```

5. **Scrape Near Partner content**
```bash
python scraper.py                 # Blog posts
python scrape_company_pages.py   # Company/product pages
```

6. **Ingest to knowledge base**
```bash
python scripts/ingest_blogs.py
```

## Usage

### Run the Main App
```bash
streamlit run app/main_app.py
```

This opens a unified interface with 4 tabs:
- **Chat**: Ask questions about Near Partner in Portuguese or English
- **Analytics**: View query logs, feedback, and performance metrics
- **ChromaDB**: Browse and search the knowledge base
- **Settings**: Configure ML features and manage the knowledge base

### Run the API Server
```bash
uvicorn src.api.main:app --reload
```

### API Endpoints

```
POST /api/v1/chat
{
  "message": "Quais sÃ£o os serviÃ§os da Near Partner?",
  "top_k": 5,
  "temperature": 0.7,
  "conversation_history": []
}

POST /api/v1/feedback
GET  /api/v1/health
GET  /api/v1/stats
GET  /api/v1/sources
```

### Export Data
```bash
python scripts/export_data.py
```
Creates a timestamped export in `data/exports/` with the knowledge base, all SQLite tables, and a manifest.

### Run Tests
```bash
pytest tests/
```

## Configuration

Edit `src/config.py`:
```python
llm_model: str = "gemma2:2b"          # LLM model
embedding_model: str = "nomic-embed-text"
top_k: int = 3                         # Chunks to retrieve
chunk_size: int = 1200                 # Characters per chunk (~300-400 tokens)
chunk_overlap: int = 200               # Overlap between chunks
api_host: str = "127.0.0.1"           # Localhost only (change to 0.0.0.0 for external)
api_port: int = 8000
```

### Settings (via UI)

| Setting | Description |
|---------|-------------|
| Query Expansion | Expand queries with related terms |
| Hybrid Search | Combine semantic + BM25 keyword search |
| HyDE | Use hypothetical document embedding |
| Response Caching | Cache responses to reduce LLM calls |
| Evaluate Confidence | Auto-score response quality (0â€“1) |
| Show Confidence | Display confidence score in chat |
| top_k | Number of chunks to retrieve |
| Temperature | LLM creativity (0.0â€“1.0) |

## Project Structure

```
NPChat/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main_app.py              # Unified Streamlit app (4 tabs)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ingest_blogs.py          # Ingestion script
â”‚   â”œâ”€â”€ export_data.py           # Data export tool
â”‚   â””â”€â”€ setup_ollama.sh          # Ollama setup script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â”œâ”€â”€ query_logger.py      # Query logging (SQLite)
â”‚   â”‚   â””â”€â”€ response_cache.py    # Response caching (SQLite)
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app + rate limiting middleware
â”‚   â”‚   â”œâ”€â”€ routes.py            # API routes (EnhancedRAGChain)
â”‚   â”‚   â””â”€â”€ schemas.py           # Pydantic schemas (with conversation history)
â”‚   â”œâ”€â”€ feedback/
â”‚   â”‚   â”œâ”€â”€ feedback_learner.py  # Automatic learning from feedback
â”‚   â”‚   â”œâ”€â”€ models.py            # Feedback models
â”‚   â”‚   â””â”€â”€ store.py             # Feedback storage
â”‚   â”œâ”€â”€ generation/
â”‚   â”‚   â”œâ”€â”€ enhanced_rag_chain.py  # Main RAG pipeline + confidence eval
â”‚   â”‚   â”œâ”€â”€ llm.py                 # Ollama wrapper
â”‚   â”‚   â””â”€â”€ prompts.py             # Portuguese prompts + injection protection
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ chunker.py           # Text chunking (character-based)
â”‚   â”‚   â”œâ”€â”€ embedder.py          # Embedding generation
â”‚   â”‚   â””â”€â”€ ingest.py            # Ingestion pipeline
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â”œâ”€â”€ hybrid_retriever.py  # Hybrid search (semantic + BM25 + RRF)
â”‚   â”‚   â”œâ”€â”€ query_expansion.py   # Query expansion + HyDE
â”‚   â”‚   â”œâ”€â”€ retriever.py         # Base semantic retriever
â”‚   â”‚   â””â”€â”€ vector_store.py      # ChromaDB wrapper
â”‚   â”œâ”€â”€ scheduler.py             # APScheduler background jobs
â”‚   â””â”€â”€ config.py                # Central configuration
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_chunker.py          # Chunker unit tests
â”‚   â”œâ”€â”€ test_prompts.py          # Prompt builder tests
â”‚   â”œâ”€â”€ test_feedback_learner.py # Feedback learning tests
â”‚   â””â”€â”€ test_response_cache.py  # Cache tests
â”œâ”€â”€ scraper.py                   # Blog post scraper
â”œâ”€â”€ scrape_company_pages.py      # Company page scraper (13 pages)
â””â”€â”€ requirements.txt
```

## Feedback Learning System

The app automatically learns from user feedback:

| Feedback | Actions Taken |
|----------|---------------|
| **ðŸ‘ Positive** | Boost chunk scores (+0.1), learn query mapping, keep cache |
| **ðŸ‘Ž Negative** | Invalidate cache, penalize chunks (-0.15), track for flagging |
| **2+ ðŸ‘Ž on same query** | Auto-flag query for human review |

View learning statistics in the **Analytics > Learning** tab.

## Automatic Scheduling

When running the Streamlit app, a background scheduler starts automatically:

| Schedule | Job |
|----------|-----|
| Monday 02:00 | Full update: scrape blog + company pages, re-ingest |
| Daily 03:00 | Clear expired cache entries |
| Sunday 23:00 | Generate weekly analytics report |

Disable in Settings or by not starting the scheduler.

## Security Considerations

- **API Binding**: Binds to `127.0.0.1` by default. Change to `0.0.0.0` in `src/config.py` for external access.
- **CORS**: Configured for localhost Streamlit. Update `src/api/main.py` for production domains.
- **Rate Limiting**: 30 requests/minute per IP on chat endpoints (in-memory sliding window).
- **Prompt Injection**: Input sanitized (max 1000 chars, null bytes removed) + system prompt instructs the model to ignore manipulation attempts.
- **No Secrets**: Uses local Ollama â€” no API keys. If adding external APIs, use `.env` with `python-dotenv`.
- **SQL**: All queries use parameterized statements (no SQL injection).

## License

MIT License

## Acknowledgments

- [Ollama](https://ollama.ai) for local LLM inference
- [ChromaDB](https://www.trychroma.com/) for vector storage
- [Streamlit](https://streamlit.io/) for the UI
- [LangChain](https://langchain.com/) for RAG utilities
- [rank-bm25](https://github.com/dorianbrown/rank_bm25) for keyword search
